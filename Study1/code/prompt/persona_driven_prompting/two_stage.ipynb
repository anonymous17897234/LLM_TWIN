{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def generate_answers(df, idx):\n",
    "    corresponding_ans = list(df.loc[idx])\n",
    "    answer_dic = {}\n",
    "    for i, ans in enumerate(corresponding_ans):\n",
    "        answer_dic[f'Q{i+1}'] = ans\n",
    "    answer_dic_str = \"\\n\".join([f\"{key}: {value}\" for key, value in answer_dic.items()])\n",
    "    return answer_dic_str"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **PHASE1 : Generating user persona description using LLM**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"YOUR_API_KEY\")\n",
    "\n",
    "def get_response_attribute(sex, age, prior_questions, prior_answers):\n",
    "\n",
    "    poa_questions = prior_questions[0:4]    # 1~4\n",
    "    poa_questions = \"\".join(poa_questions)\n",
    "\n",
    "    coa_questions = prior_questions[4:8]    # 5~8\n",
    "    coa_questions = \"\".join(coa_questions)\n",
    "\n",
    "    eoa_questions = prior_questions[8:11]   # 9~11\n",
    "    eoa_questions = \"\".join(eoa_questions)\n",
    "\n",
    "    ioa_questions = prior_questions[11:15]  # 12~15\n",
    "    ioa_questions = \"\".join(ioa_questions)\n",
    "\n",
    "    oac_questions = prior_questions[15:19]  # 16~19\n",
    "    oac_questions = \"\".join(oac_questions)\n",
    "\n",
    "    items = re.findall(r\"(Q\\d+): ([^\\n]+)\", prior_answers)\n",
    "\n",
    "    poa_answers = str({q: a for q, a in items if 1 <= int(q[1:]) <= 4})\n",
    "    coa_answers = str({q: a for q, a in items if 5 <= int(q[1:]) <= 8})\n",
    "    eoa_answers = str({q: a for q, a in items if 9 <= int(q[1:]) <= 11})\n",
    "    ioa_answers = str({q: a for q, a in items if 12 <= int(q[1:]) <= 15})\n",
    "    oac_answers = str({q: a for q, a in items if 16 <= int(q[1:]) <= 19})\n",
    "    \n",
    "\n",
    "    role = \"\"\"\n",
    "        Describe the respondent based on the prior responses. The respondent is a Spanish netizen, answering the survey in early 2017.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        **Data Provided:**\n",
    "\n",
    "        The survey respondent is {sex} and his(or her) age is {age}.\n",
    "        The respondent answered in likert scale (1 = Completely disagree to 7 = Completely agree, 4 = Neutral)\n",
    "\n",
    "        **Survey Sections and Responses:**\n",
    "\n",
    "        1. **Pleasure induced by online advertising**\n",
    "        - Survey questions:\n",
    "            {poa_questions}\n",
    "        - Respondent's answers:\n",
    "            {poa_answers}\n",
    "\n",
    "        2. **Perceived credibility of online advertising**\n",
    "        - Survey questions:\n",
    "            {coa_questions}\n",
    "        - Respondent's answers:\n",
    "            {coa_answers}\n",
    "\n",
    "        3. **Economic evaluation of online advertising**\n",
    "        - Survey questions:\n",
    "            {eoa_questions}\n",
    "        - Respondent's answers:\n",
    "            {eoa_answers}\n",
    "\n",
    "        4. **Perceived intrusiveness of online advertising**\n",
    "        - Survey questions:\n",
    "            {ioa_questions}\n",
    "        - Respondent's answers:\n",
    "            {ioa_answers}\n",
    "\n",
    "        5. **Perceived online advertising clutter**\n",
    "        - Survey questions:\n",
    "            {oac_questions}\n",
    "        - Respondent's answers:\n",
    "            {oac_answers}\n",
    "\n",
    "        **Instructions:**\n",
    "        Consider general perspectives associated with the age and gender, along with prior responses\n",
    "        - Analyze the questions and answers carefully, aiming to provide insightful summaries that reflect the respondentâ€™s likely views and attitudes.\n",
    "        - Format your response as a text with 5 bullet points. Each bullet point should briefly summarize the respondent's attitude toward each latent variable.\n",
    "\n",
    "        **Example Response Format:**\n",
    "        [\n",
    "            - Regarding Pleasure induced by online advertising, ~~\n",
    "            - Regarding Perceived credibility of online advertising, ~~\n",
    "            - Regarding Economic evaluation of online advertising, ~~\n",
    "            - Regarding Perceived intrusiveness of online advertising, ~~\n",
    "            - Regarding Perceived online advertising clutter, ~~\n",
    "        ]\n",
    "        \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": role},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    result = response.choices[0].message.content\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "with open('../../../data/prior_questions.txt', 'r') as file:\n",
    "    prior_questions = file.readlines()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "prior_df = pd.read_csv('../../../data/dv_total.csv', index_col=0)\n",
    "target_df = pd.read_csv('../../../data/idv_total.csv', index_col=0)\n",
    "\n",
    "idx_list = list(prior_df.index)\n",
    "if idx_list != list(target_df.index):\n",
    "    print(\"Error!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "progress = 0\n",
    "descriptions_list = {}\n",
    "\n",
    "for idx in idx_list:\n",
    "    progress+=1\n",
    "    if progress%50 == 0:\n",
    "        print(f\"progres {progress} done\")\n",
    "\n",
    "    sex = prior_df.loc[idx, 'Sex']\n",
    "    age = prior_df.loc[idx, 'Age']\n",
    "\n",
    "    prior_answers = generate_answers(prior_df.iloc[:, 2:], idx) ## provided prior questions and answers\n",
    "    gpt_summary = get_response_attribute(sex, age, prior_questions, prior_answers) ## generated user persona description\n",
    "\n",
    "    descriptions_list[idx] = gpt_summary"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "description_df = pd.DataFrame.from_dict(descriptions_list, orient='index', columns=['description'])\n",
    "final_description_df = pd.concat([prior_df[['Sex', 'Age']], description_df], axis=1)\n",
    "save_path = 'PATH_TO_SAVE_PERSONA_DESCRIPTON'\n",
    "final_description_df.to_csv(f'{save_path}description.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **PHASE2: Generate responses using USER PERSONA (generated by LLM)**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"YOUR_API_KEY\")\n",
    "\n",
    "def get_survey_response(description, questions_to_answer):\n",
    "    role=f\"\"\"\n",
    "        You are a Spanish netizen, answering this survey in early 2017. Answer all questions based on the given description to maintain consisteny with the respondent. \n",
    "        \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "        **Data Provided**\n",
    "\n",
    "        Description about respondent's attitudes toward online advertising:\n",
    "        {description}\n",
    "\n",
    "        **Instructions:**\n",
    "\n",
    "        Consider general perspectives associated with your age and gender, along with you attitudes described in the given description\n",
    "        - Answer each question considering general perspectives associated with your age and gender, along with the description.\n",
    "        - Format each response as follows: \"'Q<number>': <Answer> (reason for answer)\". Strictly adhere to the required response format without adding extra text or elaboration outside this structure.\n",
    "\n",
    "        **Survey Questions to Answer:**\n",
    "        {questions_to_answer}\n",
    "\n",
    "        ***Example Response:**\n",
    "        [\n",
    "            'Q1': 7 (The respondent shows a very strong positive attitude towards this statement based on prior answers),\n",
    "            'Q2': 1 (Indicating strong skepticism or disagreement with online ad reliability based on demographic information),\n",
    "            'Q3': 4 (Showing a neutral stance as in prior responses)\n",
    "        ]\n",
    "\n",
    "        Use a Likert scale from 1 to 7 (1 = Completely disagree to 7 = Completely agree) for each answer.\n",
    "        Respond in a structured format, outlining each step to form a well-supported evaluation.\n",
    "        \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": role},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    result = response.choices[0].message.content\n",
    "    return result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "desc_path = 'DESCRIPTION_PATH'\n",
    "description_df = pd.read_csv(f'{desc_path}description.csv', index_col=0)\n",
    "with open(f\"../../questions_to_answer.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    questions_to_answer = file.read()\n",
    "target_df = pd.read_csv('../../idv_total.csv', index_col=0)\n",
    "\n",
    "save_path = 'PATH_TO_SAVE_RESPONSES'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "progress = 0\n",
    "answer_list = {}\n",
    "\n",
    "for idx in idx_list:\n",
    "    progress+=1\n",
    "    if progress%50 == 0:\n",
    "        print(f\"progres {progress} done\")\n",
    "\n",
    "    sex = description_df.loc[idx, 'Sex']\n",
    "    age = description_df.loc[idx, 'Age']\n",
    "    description = description_df.loc[idx, 'description']\n",
    "\n",
    "    gpt_answers_raw = get_survey_response(description, questions_to_answer) ## responses generated by LLM\n",
    "    gpt_answers = re.findall(r\"[\\\"']Q\\d+[\\\"']:\\s*(\\d+)\", gpt_answers_raw)\n",
    "    gpt_answers = list(map(int, gpt_answers))\n",
    "    \n",
    "    human_answers_with_Q = generate_answers(target_df, idx) ## Original human responses\n",
    "    human_answers = re.findall(r': (\\d+)', str(human_answers_with_Q))\n",
    "    human_answers = list(map(int, human_answers))\n",
    "\n",
    "    answer_list[idx] = gpt_answers\n",
    "\n",
    "    if len(gpt_answers) == len(human_answers):\n",
    "        check = [1 if gpt_answers[i] == human_answers[i] else 0 for i in range(len(gpt_answers))]\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"raw_answer\": gpt_answers_raw,\n",
    "            \"gpt_answers\": gpt_answers,\n",
    "            \"human_answers\": human_answers,\n",
    "            \"check\": check\n",
    "        })\n",
    "        df.to_csv(f'{save_path}idx_{idx}_result.csv', index=False)\n",
    "    \n",
    "    else:\n",
    "        print(f\"The lengths of gpt answers and human answers for {idx} do not match.\")\n",
    "        df = pd.DataFrame({\n",
    "            \"raw_answer\": gpt_answers_raw,\n",
    "            \"human_answers\": human_answers,\n",
    "        })\n",
    "        df.to_csv(f'{save_path}idx_{idx}_result.csv', index=False)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "gpt_aoa = pd.DataFrame.from_dict(answer_list, orient=\"index\", columns=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"])\n",
    "gpt_aoa.to_csv(f'{save_path}llm_responses.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}